
# Yocto Devcontainer for BeagleBone Black (BBB)  
A complete, containerized workflow for building **Yocto Kirkstone images** for **BeagleBone Black (AM335x)** on macOS using **Docker + VSCode Devcontainers**.

---

## ğŸ“Œ Overview  

This repository contains:

- A **VSCode devcontainer** that sets up a full Yocto build environment  
- Docker-based **reproducible development environment**  
- **Shared cache directories by default** â€” multiple instances share downloads and sstate cache  
- Preconfigured **DL_DIR** and **SSTATE_DIR** mounts for caching  
- Instructions to build **core-image-minimal** for BeagleBone Black  

---

## ğŸš€ Features  

âœ” Works on **macOS (Intel or M-series)**  
âœ” No Yocto installation required on host  
âœ” Safe, self-contained build environment  
âœ” Supports **meta-ti** for AM335x (BBB)  
âœ” Rebuilds are extremely fast using **sstate + download cache**  
âœ” **Shared caches enabled by default** â€” multiple instances share downloads and sstate cache  
âœ” Clean and reproducible builds  

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json
â”œâ”€â”€ .env                    â† Pre-configured for shared caches
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ workspace/
â”‚   â””â”€â”€ (Yocto source + builds will appear here)
â”œâ”€â”€ sstate/                 â† Local cache (fallback)
â”œâ”€â”€ downloads/              â† Local cache (fallback)
â””â”€â”€ README.md   â† you are here
```

---

# ğŸ³ 1. Requirements

| Component | Required |
|----------|----------|
| macOS 12+ | âœ” |
| Docker Desktop | âœ” |
| VSCode + Dev Containers extension | âœ” |
| 50â€“120 GB disk space | âœ” |
| 8â€“16 GB RAM (recommended) | âœ” |

---

# ğŸ§° 2. Setup Instructions (From Zero)

## **Step 1 â€” Clone this repo**

```bash
git clone https://github.com/hassansys2/yocto-beaglebone-devcontainer-template.git
cd yocto-beaglebone-devcontainer-template
```

---

## **Step 2 â€” Create Shared Cache Directories (One-time setup)**

This repository is configured to use **shared cache directories** by default (via `.env` file). This allows multiple instances to share downloads and sstate cache, saving disk space and speeding up builds.

Create the shared cache directories:

```bash
mkdir -p ~/yocto-shared/sstate
mkdir -p ~/yocto-shared/downloads
```

**Note:** The `.env` file is already configured to use these directories. If you prefer per-instance caches, see [Section 8.1](#81-using-per-instance-caches-optional) for instructions.

---

## **Step 3 â€” Launch Devcontainer**

Open folder in VSCode â†’  
**Command Palette â†’ "Dev Containers: Reopen in Container"**

VSCode will:

- Build Docker image  
- Launch devcontainer  
- Mount workspace + **shared caches** (from `~/yocto-shared/`)  
- Configure Yocto environment  

The `.env` file is automatically loaded by Docker Compose, so shared caches are used by default.

---

## **Step 4 â€” Download Yocto (Kirkstone)**

Inside container:

```bash
cd /workspace
git clone --branch kirkstone git://git.yoctoproject.org/poky
git clone --branch kirkstone https://github.com/openembedded/meta-openembedded.git
git clone --branch kirkstone https://git.yoctoproject.org/meta-arm
git clone --branch kirkstone https://git.yoctoproject.org/meta-ti.git
```

---

## **Step 5 â€” Create Yocto Build Directory**

```bash
source poky/oe-init-build-env build-bbb-kirkstone
```

This creates:

```
build-bbb-kirkstone/
â”œâ”€â”€ conf/bblayers.conf
â”œâ”€â”€ conf/local.conf
```

---

## **Step 6 â€” Configure MACHINE**

Edit:

```
conf/local.conf
```

Set:

```conf
MACHINE = "am335x-evm"
DL_DIR ?= "/downloads"
SSTATE_DIR ?= "/sstate"
TMPDIR = "/tmp/yocto/tmp-bbb-kirkstone"
```

---

## **Step 7 â€” Add Required Layers**

```bash
bitbake-layers add-layer /workspace/meta-openembedded/meta-oe
bitbake-layers add-layer /workspace/meta-openembedded/meta-python
bitbake-layers add-layer /workspace/meta-openembedded/meta-networking
bitbake-layers add-layer /workspace/meta-arm/meta-arm-toolchain
bitbake-layers add-layer /workspace/meta-arm/meta-arm
bitbake-layers add-layer /workspace/meta-ti/meta-ti-bsp
bitbake-layers add-layer /workspace/meta-ti/meta-ti-extras
```

---

## **Step 8 â€” Build Yocto Image**

```bash
bitbake core-image-minimal
```

After build:

Images appear at:

```
build-bbb-kirkstone/deploy-ti/images/am335x-evm/
```

Important output files:

| File | Purpose |
|------|---------|
| `.wic.xz` | SD card image |
| `MLO` | SPL bootloader |
| `u-boot.img` | U-Boot |
| `zImage` | Kernel |
| `.dtb` | Device tree files |

---

# ğŸ”¥ 3. Flashing Image to SD Card (macOS)

```bash
xz -d core-image-minimal-am335x-evm.wic.xz
diskutil list
diskutil unmountDisk /dev/diskX
sudo dd if=core-image-minimal-am335x-evm.wic of=/dev/rdiskX bs=4m status=progress
sync
```

Insert SD into BBB and boot.

---

# ğŸŒ± 4. Understanding Yocto Internals (Concept Summary)

### **MACHINE selection**
- Yocto reads `conf/machine/<machine>.conf`
- Includes kernel, SPL, U-Boot config, tuning

For example:

```
meta-ti/meta-ti-bsp/conf/machine/am335x-evm.conf
```

---

### **Layers**
Each layer has:

```
conf/layer.conf
recipes-*/...
dynamic-layers/
classes/
```

Yocto loads layers based on `bblayers.conf`.

---

### **Recipe Execution Order**

Bitbake tasks:

```
do_fetch â†’ do_unpack â†’ do_patch â†’ do_configure â†’ do_compile â†’ do_install â†’ do_package â†’ do_rootfs â†’ do_image
```

---

### **Images**

`core-image-minimal.bb` defines:

- Busybox
- Init manager
- Basic kernel modules
- Root filesystem packaging  

---

# ğŸ§© 5. Adding Your Custom Layer (meta-hassan)

```bash
cd /workspace
bitbake-layers create-layer meta-hassan
bitbake-layers add-layer meta-hassan
```

Create custom recipes:

```
meta-hassan/recipes-apps/myapp/myapp.bb
```

---

# ğŸ“„ 6. Included Devcontainer Files

### **devcontainer.json**

```json
{
  "name": "Yocto (BBB) â€” Dev Container",
  "dockerComposeFile": ["../docker-compose.yml"],
  "service": "yocto",
  "workspaceFolder": "/workspace",
  "remoteUser": "dev",
  "overrideCommand": false,
  "customizations": {
    "vscode": {
      "settings": {
        "terminal.integrated.defaultProfile.linux": "bash",
        "files.exclude": {
          "**/build": true,
          "**/tmp": true
        },
        "editor.tabSize": 2
      },
      "extensions": [
        "ms-vscode.cpptools",
        "ms-vscode.cmake-tools",
        "ms-azuretools.vscode-docker",
        "eamodio.gitlens",
        "github.vscode-pull-request-github"
      ]
    }
  },
  "postCreateCommand": "bash -lc 'git config --global --add safe.directory /workspace && sudo chown -R dev:dev /workspace /sstate /downloads || true && echo \"Devcontainer ready!\"'"
}
```

---

### **docker-compose.yml**

```yaml
services:
  yocto:
    build:
      context: .
      dockerfile: Dockerfile
    image: emb-linux:22.04
    tty: true
    stdin_open: true
    working_dir: /workspace
    environment:
      - LANG=en_US.UTF-8
      - LC_ALL=en_US.UTF-8
      - CCACHE_DIR=/home/dev/.ccache
      - DL_DIR=/downloads
      - SSTATE_DIR=/sstate
    volumes:
      - ./workspace:/workspace
      # Cache directories: Shared caches are enabled by default via .env file
      # Falls back to local ./sstate and ./downloads if env vars are not set
      - ${YOCTO_SHARED_SSTATE_DIR:-./sstate}:/sstate
      - ${YOCTO_SHARED_DOWNLOADS_DIR:-./downloads}:/downloads
    # Resource limits: Uncomment and adjust values to limit resource usage
    # Useful when running multiple instances simultaneously
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '4'
    #       memory: 8G
    #     reservations:
    #       cpus: '2'
    #       memory: 4G
    command: bash -lc "bash"
```

---

### **Dockerfile**

```Dockerfile
FROM ubuntu:22.04

ENV DEBIAN_FRONTEND=noninteractive
RUN apt-get update && apt-get install -y --no-install-recommends \
    bash-completion build-essential bc bison flex libssl-dev \
    libncurses5-dev libncursesw5-dev libelf-dev \
    gawk wget git curl ca-certificates rsync file xz-utils zstd \
    cpio unzip zip tar python3 python3-pip python3-venv python3-distutils \
    locales sudo pkg-config cmake ninja-build meson \
    u-boot-tools device-tree-compiler \
    texinfo chrpath diffstat socat bc \
    liblz4-tool zlib1g-dev \
    ccache ssh pass vim nano tmux \
    && rm -rf /var/lib/apt/lists/*

RUN locale-gen en_US.UTF-8
ENV LANG=en_US.UTF-8 LANGUAGE=en_US:en LC_ALL=en_US.UTF-8

ARG USER=dev
ARG UID=1000
ARG GID=1000
RUN groupadd -g ${GID} ${USER} && \
    useradd -m -u ${UID} -g ${GID} -G sudo -s /bin/bash ${USER} && \
    echo "${USER} ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/99_${USER}

RUN mkdir -p /home/dev/.ccache

USER dev
WORKDIR /workspace
```

---

# ğŸ©µ 7. Troubleshooting

### **Build too slow?**
Use global mirrors:

```
INHERIT += "own-mirrors"
SOURCE_MIRROR_URL = "https://downloads.yoctoproject.org/mirror/sources/"
BB_FETCH_PREMIRRORONLY = "1"
```

---

### **macOS Sleep Issue**
Prevent mac sleep during build:

```bash
caffeinate -dims &
```

---

# ğŸ”„ 8. Running Multiple Instances

This repository is **pre-configured for multiple instances** with shared cache directories. All instances automatically share downloads and sstate cache, saving disk space and speeding up builds.

---

## **8.1 Shared Caches (Default Configuration)**

**Shared caches are enabled by default** via the included `.env` file:

```bash
YOCTO_SHARED_SSTATE_DIR=${HOME}/yocto-shared/sstate
YOCTO_SHARED_DOWNLOADS_DIR=${HOME}/yocto-shared/downloads
```

### **Benefits:**
- âœ… **Save disk space** â€” downloads are shared across all instances
- âœ… **Faster builds** â€” sstate cache is shared (rebuilds are much faster)
- âœ… **Reduce bandwidth** â€” downloads happen once, reused by all instances
- âœ… **No configuration needed** â€” works out of the box

### **How It Works:**
1. Each instance clones this repository (with `.env` file)
2. All instances point to the same shared cache directories (`~/yocto-shared/`)
3. First build downloads everything, subsequent builds reuse cache
4. Each instance maintains its own `workspace/` directory (builds are isolated)

### **Example: Multiple Projects**

```bash
# Project A
cd ~/projects/yocto-bbb-project-a
git clone https://github.com/hassansys2/yocto-beaglebone-devcontainer-template.git .
# .env file is already included - shared caches work automatically!

# Project B  
cd ~/projects/yocto-bbb-project-b
git clone https://github.com/hassansys2/yocto-beaglebone-devcontainer-template.git .
# Also uses shared caches automatically!
```

Both projects will share the same cache, but have separate build workspaces.

---

## **8.1.1 Using Per-Instance Caches (Optional)**

If you need **isolated caches** per instance (e.g., different Yocto versions, testing cache behavior), you can disable shared caches:

### **Option 1: Remove or rename .env file**

```bash
# Rename .env to disable shared caches
mv .env .env.disabled
```

The instance will fall back to local `./sstate` and `./downloads` directories.

### **Option 2: Modify .env file**

Edit `.env` and comment out or remove the shared cache variables:

```bash
# YOCTO_SHARED_SSTATE_DIR=${HOME}/yocto-shared/sstate
# YOCTO_SHARED_DOWNLOADS_DIR=${HOME}/yocto-shared/downloads
```

The instance will use local per-instance caches.

---

## **8.2 Resource Limits**

When running multiple instances simultaneously, limit resource usage to prevent system overload.

### **Enable Resource Limits**

Edit `docker-compose.yml` and uncomment the `deploy.resources` section:

```yaml
deploy:
  resources:
    limits:
      cpus: '4'      # Limit to 4 CPU cores
      memory: 8G     # Limit to 8GB RAM
    reservations:
      cpus: '2'      # Reserve 2 CPU cores
      memory: 4G     # Reserve 4GB RAM
```

**Recommended limits per instance:**
- **CPU**: 2-4 cores (adjust based on total CPU cores)
- **Memory**: 6-8GB (Yocto builds are memory-intensive)

---

## **8.3 Best Practices**

### **âœ… Do:**
- **Use shared caches** (default) â€” all instances automatically share caches
- Set resource limits when running 2+ instances simultaneously
- Monitor disk space (shared cache + each build workspace needs 50-120GB)
- Use different `workspace/` directories per instance (already isolated)
- Create shared cache directories once (Step 2 in setup)

### **âŒ Don't:**
- Run more than 2-3 instances simultaneously (unless you have 32+ GB RAM)
- Share caches between different Yocto versions (e.g., Kirkstone vs. Dunfell) â€” use per-instance caches for different versions
- Share `workspace/` directories (builds must be isolated)
- Delete `.env` file unless you need per-instance caches

---

## **8.4 Example: Two Instances**

**Instance 1** (Project A):
```bash
cd ~/projects/yocto-bbb-project-a
git clone https://github.com/hassansys2/yocto-beaglebone-devcontainer-template.git .
# .env file is included - shared caches configured automatically!
# Open in VSCode â†’ Reopen in Container
```

**Instance 2** (Project B):
```bash
cd ~/projects/yocto-bbb-project-b
git clone https://github.com/hassansys2/yocto-beaglebone-devcontainer-template.git .
# .env file is included - shared caches configured automatically!
# Open in VSCode â†’ Reopen in Container
```

**No additional configuration needed!** Both instances will:
- âœ… Share sstate cache (faster rebuilds)
- âœ… Share downloads (saves disk/bandwidth)
- âœ… Use separate workspaces (no build conflicts)
- âœ… Work out of the box (`.env` file handles everything)

---

## **8.5 Resource Usage Estimate**

| Scenario | Instances | Total RAM | Total Disk | Build Time |
|----------|-----------|-----------|------------|------------|
| Single instance (shared cache) | 1 | ~12GB | ~80GB | 4 hours |
| Two instances (shared cache - **default**) | 2 | ~24GB | ~100GB | 6-8 hours |
| Two instances (separate cache) | 2 | ~24GB | ~160GB | 8-10 hours |

**Note:** 
- Shared cache (default) saves ~60GB disk space for two instances
- Build times increase due to resource contention when running multiple instances
- First build downloads everything, subsequent builds are much faster due to shared cache

---

# ğŸ‰ 9. You're Ready!

You now have a **full Yocto build environment** on macOS using Docker.

---

# ğŸ“œ License  
MIT â€” free for learning & commercial usage.

---
