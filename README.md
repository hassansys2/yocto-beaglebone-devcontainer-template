
# Yocto Devcontainer for BeagleBone Black (BBB)  
A complete, containerized workflow for building **Yocto Kirkstone images** for **BeagleBone Black (AM335x)** on macOS using **Docker + VSCode Devcontainers**.

---

## üìå Overview  

This repository contains:

- A **VSCode devcontainer** that sets up a full Yocto build environment  
- Docker-based **reproducible development environment**  
- **Shared cache directories by default** ‚Äî multiple instances share downloads and sstate cache  
- Preconfigured **DL_DIR** and **SSTATE_DIR** mounts for caching  
- Instructions to build **core-image-minimal** for BeagleBone Black  

---

## üöÄ Features  

‚úî Works on **macOS (Intel or M-series)**  
‚úî No Yocto installation required on host  
‚úî Safe, self-contained build environment  
‚úî Supports **meta-ti** for AM335x (BBB)  
‚úî Rebuilds are extremely fast using **sstate + download cache**  
‚úî **Shared caches enabled by default** ‚Äî multiple instances share downloads and sstate cache  
‚úî Clean and reproducible builds  

---

## üìÇ Repository Structure

```
.
‚îú‚îÄ‚îÄ .devcontainer/
‚îÇ   ‚îî‚îÄ‚îÄ devcontainer.json
‚îú‚îÄ‚îÄ .env                    ‚Üê Pre-configured for shared caches
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ workspace/
‚îÇ   ‚îî‚îÄ‚îÄ (Yocto source + builds will appear here)
‚îú‚îÄ‚îÄ sstate/                 ‚Üê Local cache (fallback)
‚îú‚îÄ‚îÄ downloads/              ‚Üê Local cache (fallback)
‚îî‚îÄ‚îÄ README.md   ‚Üê you are here
```

---

# üê≥ 1. Requirements

| Component | Required |
|----------|----------|
| macOS 12+ | ‚úî |
| Docker Desktop | ‚úî |
| VSCode + Dev Containers extension | ‚úî |
| 50‚Äì120 GB disk space | ‚úî |
| 8‚Äì16 GB RAM (recommended) | ‚úî |

---

# üß∞ 2. Setup Instructions (From Zero)

## **Step 1 ‚Äî Clone this repo**

```bash
git clone https://github.com/hassansys2/yocto-beaglebone-devcontainer-template.git
cd yocto-beaglebone-devcontainer-template
```

---

## **Step 2 ‚Äî Create Shared Cache Directories (One-time setup)**

This repository is configured to use **shared cache directories** by default (via `.env` file). This allows multiple instances to share downloads and sstate cache, saving disk space and speeding up builds.

Create the shared cache directories:

```bash
mkdir -p ~/yocto-shared/sstate
mkdir -p ~/yocto-shared/downloads
```

**Note:** The `.env` file is already configured to use these directories. If you prefer per-instance caches, see [Section 9.1.1](#911-using-per-instance-caches-optional) for instructions.

---

## **Step 3 ‚Äî Launch Devcontainer**

Open folder in VSCode ‚Üí  
**Command Palette ‚Üí "Dev Containers: Reopen in Container"**

VSCode will:

- Build Docker image  
- Launch devcontainer  
- Mount workspace + **shared caches** (from `~/yocto-shared/`)  
- Configure Yocto environment  

The `.env` file is automatically loaded by Docker Compose, so shared caches are used by default.

---

## **Step 4 ‚Äî Download Yocto (Kirkstone)**

Inside container:

```bash
cd /workspace
git clone --branch kirkstone git://git.yoctoproject.org/poky
git clone --branch kirkstone https://github.com/openembedded/meta-openembedded.git
git clone --branch kirkstone https://git.yoctoproject.org/meta-arm
git clone --branch kirkstone https://git.yoctoproject.org/meta-ti.git
```

---

## **Step 5 ‚Äî Create Yocto Build Directory**

```bash
source poky/oe-init-build-env build-bbb-kirkstone
```

This creates:

```
build-bbb-kirkstone/
‚îú‚îÄ‚îÄ conf/bblayers.conf
‚îú‚îÄ‚îÄ conf/local.conf
```

---

## **Step 6 ‚Äî Configure MACHINE**

Edit:

```
conf/local.conf
```

Set:

```conf
MACHINE = "am335x-evm"
DL_DIR ?= "/downloads"
SSTATE_DIR ?= "/sstate"
TMPDIR = "/tmp/yocto/tmp-bbb-kirkstone"

# Optional: Tune build parallelism based on your Mac's CPU cores and RAM
# BB_NUMBER_THREADS ?= "4"
# PARALLEL_MAKE ?= "-j4"
```

**Note:** Uncomment and adjust `BB_NUMBER_THREADS` and `PARALLEL_MAKE` based on your Mac's CPU cores and available RAM. A good starting point is half your CPU cores (e.g., 4 for an 8-core Mac).

---

## **Step 7 ‚Äî Add Required Layers**

```bash
bitbake-layers add-layer /workspace/meta-openembedded/meta-oe
bitbake-layers add-layer /workspace/meta-openembedded/meta-python
bitbake-layers add-layer /workspace/meta-openembedded/meta-networking
bitbake-layers add-layer /workspace/meta-arm/meta-arm-toolchain
bitbake-layers add-layer /workspace/meta-arm/meta-arm
bitbake-layers add-layer /workspace/meta-ti/meta-ti-bsp
bitbake-layers add-layer /workspace/meta-ti/meta-ti-extras
```

**Verify layers are added correctly:**

Check `conf/bblayers.conf` ‚Äî it should look like:

```conf
BBLAYERS ?= " \
  /workspace/poky/meta \
  /workspace/poky/meta-poky \
  /workspace/poky/meta-yocto-bsp \
  /workspace/meta-openembedded/meta-oe \
  /workspace/meta-openembedded/meta-python \
  /workspace/meta-openembedded/meta-networking \
  /workspace/meta-arm/meta-arm-toolchain \
  /workspace/meta-arm/meta-arm \
  /workspace/meta-ti/meta-ti-bsp \
  /workspace/meta-ti/meta-ti-extras \
"
```

---

## **Step 8 ‚Äî Build Yocto Image**

```bash
bitbake core-image-minimal
```

After build:

**Image Output Location:**

Depending on meta-ti / SDK configuration, images may appear under either:

- `build-bbb-kirkstone/tmp/deploy/images/am335x-evm/`, or  
- `build-bbb-kirkstone/deploy-ti/images/am335x-evm/`

To locate the deploy directory:

```bash
find build-bbb-kirkstone -maxdepth 4 -type d -name "images" -print
```

**Important output files:**

| File | Purpose |
|------|---------|
| `.wic.xz` | SD card image |
| `MLO` | SPL bootloader |
| `u-boot.img` | U-Boot |
| `zImage` | Kernel |
| `.dtb` | Device tree files |

---

# üî• 3. Flashing Image to SD Card (macOS)

```bash
xz -d core-image-minimal-am335x-evm.wic.xz
diskutil list
diskutil unmountDisk /dev/diskX
sudo dd if=core-image-minimal-am335x-evm.wic of=/dev/rdiskX bs=4m status=progress
sync
```

Insert SD into BBB and boot.

---

# üå± 4. Understanding Yocto Internals (Concept Summary)

### **MACHINE selection**
- Yocto reads `conf/machine/<machine>.conf`
- Includes kernel, SPL, U-Boot config, tuning

For example:

```
meta-ti/meta-ti-bsp/conf/machine/am335x-evm.conf
```

---

### **Layers**
Each layer has:

```
conf/layer.conf
recipes-*/...
dynamic-layers/
classes/
```

Yocto loads layers based on `bblayers.conf`.

---

### **Recipe Execution Order**

Bitbake tasks:

```
do_fetch ‚Üí do_unpack ‚Üí do_patch ‚Üí do_configure ‚Üí do_compile ‚Üí do_install ‚Üí do_package ‚Üí do_rootfs ‚Üí do_image
```

---

### **Images**

`core-image-minimal.bb` defines:

- Busybox
- Init manager
- Basic kernel modules
- Root filesystem packaging  

---

# üîß 5. Changing MACHINE / DISTRO

## **Changing MACHINE**

To build for a different TI board or AM335x variant, edit `conf/local.conf`:

```conf
MACHINE = "am335x-evm"  # Change to your target machine
```

**Supported AM335x machines in meta-ti:**
- `am335x-evm` ‚Äî BeagleBone Black (default)
- `am335x-evm-reva` ‚Äî BeagleBone Black Rev A
- Other AM335x variants as supported by meta-ti

**Find available machines:**

```bash
# List all machines in meta-ti
ls /workspace/meta-ti/meta-ti-bsp/conf/machine/

# Or search for AM335x machines
find /workspace/meta-ti -name "*.conf" -path "*/machine/*" | grep -i am335x
```

**Note:** The `am335x-evm` machine configuration also covers BeagleBone Black. See [TI's Yocto/Processor SDK documentation](https://software-dl.ti.com/processor-sdk-linux/esd/docs/latest/linux/Overview_Building_the_SDK.html) for other supported machines.

---

## **Changing DISTRO**

By default, this setup uses Poky's `poky` distribution. To use TI's Processor SDK / Arago-style distribution:

1. **Add meta-arago layer** (if not already present):

```bash
bitbake-layers add-layer /workspace/meta-ti/meta-arago
```

2. **Set DISTRO in `conf/local.conf`:**

```conf
DISTRO = "arago"  # or "arago-tiny", "arago-base", etc.
```

3. **Check available distributions:**

```bash
ls /workspace/meta-ti/meta-arago/conf/distro/ 2>/dev/null || echo "meta-arago not found"
```

**Note:** Different DISTROs may require different layer combinations. Refer to TI's Processor SDK documentation for specific requirements.

---

# üß© 6. Adding Your Custom Layer (meta-hassan)

```bash
cd /workspace
bitbake-layers create-layer meta-hassan
bitbake-layers add-layer meta-hassan
```

Create custom recipes:

```
meta-hassan/recipes-apps/myapp/myapp.bb
```

---

# üìÑ 7. Included Devcontainer Files

### **devcontainer.json**

```json
{
  "name": "Yocto (BBB) ‚Äî Dev Container",
  "dockerComposeFile": ["../docker-compose.yml"],
  "service": "yocto",
  "workspaceFolder": "/workspace",
  "remoteUser": "dev",
  "overrideCommand": false,
  "customizations": {
    "vscode": {
      "settings": {
        "terminal.integrated.defaultProfile.linux": "bash",
        "files.exclude": {
          "**/build": true,
          "**/tmp": true
        },
        "editor.tabSize": 2
      },
      "extensions": [
        "ms-vscode.cpptools",
        "ms-vscode.cmake-tools",
        "ms-azuretools.vscode-docker",
        "eamodio.gitlens",
        "github.vscode-pull-request-github"
      ]
    }
  },
  "postCreateCommand": "bash -lc 'git config --global --add safe.directory /workspace && sudo chown -R dev:dev /workspace /sstate /downloads || true && echo \"Devcontainer ready!\"'"
}
```

---

### **docker-compose.yml**

```yaml
services:
  yocto:
    build:
      context: .
      dockerfile: Dockerfile
    image: emb-linux:22.04
    tty: true
    stdin_open: true
    working_dir: /workspace
    environment:
      - LANG=en_US.UTF-8
      - LC_ALL=en_US.UTF-8
      - CCACHE_DIR=/home/dev/.ccache
      - DL_DIR=/downloads
      - SSTATE_DIR=/sstate
    volumes:
      - ./workspace:/workspace
      # Cache directories: Shared caches are enabled by default via .env file
      # Falls back to local ./sstate and ./downloads if env vars are not set
      - ${YOCTO_SHARED_SSTATE_DIR:-./sstate}:/sstate
      - ${YOCTO_SHARED_DOWNLOADS_DIR:-./downloads}:/downloads
    # Resource limits: Uncomment and adjust values to limit resource usage
    # Useful when running multiple instances simultaneously
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '4'
    #       memory: 8G
    #     reservations:
    #       cpus: '2'
    #       memory: 4G
    command: bash -lc "bash"
```

---

### **Dockerfile**

```Dockerfile
FROM ubuntu:22.04

ENV DEBIAN_FRONTEND=noninteractive
RUN apt-get update && apt-get install -y --no-install-recommends \
    bash-completion build-essential bc bison flex libssl-dev \
    libncurses5-dev libncursesw5-dev libelf-dev \
    gawk wget git curl ca-certificates rsync file xz-utils zstd \
    cpio unzip zip tar python3 python3-pip python3-venv python3-distutils \
    locales sudo pkg-config cmake ninja-build meson \
    u-boot-tools device-tree-compiler \
    texinfo chrpath diffstat socat bc \
    liblz4-tool zlib1g-dev \
    ccache ssh pass vim nano tmux \
    && rm -rf /var/lib/apt/lists/*

RUN locale-gen en_US.UTF-8
ENV LANG=en_US.UTF-8 LANGUAGE=en_US:en LC_ALL=en_US.UTF-8

ARG USER=dev
ARG UID=1000
ARG GID=1000
RUN groupadd -g ${GID} ${USER} && \
    useradd -m -u ${UID} -g ${GID} -G sudo -s /bin/bash ${USER} && \
    echo "${USER} ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/99_${USER}

RUN mkdir -p /home/dev/.ccache

USER dev
WORKDIR /workspace
```

---

# ü©µ 8. Troubleshooting

### **Build too slow?**
Use global mirrors:

```
INHERIT += "own-mirrors"
SOURCE_MIRROR_URL = "https://downloads.yoctoproject.org/mirror/sources/"
BB_FETCH_PREMIRRORONLY = "1"
```

---

### **macOS Sleep Issue**
Prevent mac sleep during build:

```bash
caffeinate -dims &
```

---

### **Disk Space & Cleanup**

Yocto builds are disk-intensive. Make sure **Docker Desktop's disk image** has at least 80‚Äì120 GB available.

**Free space inside a project:**

```bash
# From inside the container
cd /workspace/build-bbb-kirkstone

# Clean sstate cache for a specific recipe
bitbake -c cleansstate core-image-minimal

# Remove temporary build files (WARNING: forces full rebuild)
rm -rf tmp

# Clean all sstate cache
bitbake -c cleansstate -a
```

**Free space on host (Docker cleanup):**

```bash
# Remove unused Docker images, containers, and volumes
docker system prune -af

# Remove unused volumes only
docker volume prune -f
```

**Monitor disk space:**

```bash
# Inside container
df -h /tmp/yocto/tmp-bbb-kirkstone

# On host
docker system df
```

**Common error:** If you see:
```
ERROR: No new tasks can be executed since the disk space monitor action is "STOPTASKS"!
```

This means Docker's disk image or the container's `/tmp` is full. Free up space using the commands above.

---

# üîÑ 9. Running Multiple Instances

This repository is **pre-configured for multiple instances** with shared cache directories. All instances automatically share downloads and sstate cache, saving disk space and speeding up builds.

---

## **9.1 Shared Caches (Default Configuration)**

**Shared caches are enabled by default** via the included `.env` file:

```bash
YOCTO_SHARED_SSTATE_DIR=${HOME}/yocto-shared/sstate
YOCTO_SHARED_DOWNLOADS_DIR=${HOME}/yocto-shared/downloads
```

### **Benefits:**
- ‚úÖ **Save disk space** ‚Äî downloads are shared across all instances
- ‚úÖ **Faster builds** ‚Äî sstate cache is shared (rebuilds are much faster)
- ‚úÖ **Reduce bandwidth** ‚Äî downloads happen once, reused by all instances
- ‚úÖ **No configuration needed** ‚Äî works out of the box

### **How It Works:**
1. Each instance clones this repository (with `.env` file)
2. All instances point to the same shared cache directories (`~/yocto-shared/`)
3. First build downloads everything, subsequent builds reuse cache
4. Each instance maintains its own `workspace/` directory (builds are isolated)

### **Example: Multiple Projects**

```bash
# Project A
cd ~/projects/yocto-bbb-project-a
git clone https://github.com/hassansys2/yocto-beaglebone-devcontainer-template.git .
# .env file is already included - shared caches work automatically!

# Project B  
cd ~/projects/yocto-bbb-project-b
git clone https://github.com/hassansys2/yocto-beaglebone-devcontainer-template.git .
# Also uses shared caches automatically!
```

Both projects will share the same cache, but have separate build workspaces.

---

## **9.1.1 Using Per-Instance Caches (Optional)**

If you need **isolated caches** per instance (e.g., different Yocto versions, testing cache behavior), you can disable shared caches:

### **Option 1: Remove or rename .env file**

```bash
# Rename .env to disable shared caches
mv .env .env.disabled
```

The instance will fall back to local `./sstate` and `./downloads` directories.

### **Option 2: Modify .env file**

Edit `.env` and comment out or remove the shared cache variables:

```bash
# YOCTO_SHARED_SSTATE_DIR=${HOME}/yocto-shared/sstate
# YOCTO_SHARED_DOWNLOADS_DIR=${HOME}/yocto-shared/downloads
```

The instance will use local per-instance caches.

---

## **9.2 Resource Limits**

When running multiple instances simultaneously, limit resource usage to prevent system overload.

### **Enable Resource Limits**

Edit `docker-compose.yml` and uncomment the `deploy.resources` section:

```yaml
deploy:
  resources:
    limits:
      cpus: '4'      # Limit to 4 CPU cores
      memory: 8G     # Limit to 8GB RAM
    reservations:
      cpus: '2'      # Reserve 2 CPU cores
      memory: 4G     # Reserve 4GB RAM
```

**Recommended limits per instance:**
- **CPU**: 2-4 cores (adjust based on total CPU cores)
- **Memory**: 6-8GB (Yocto builds are memory-intensive)

---

## **9.3 Best Practices**

### **‚úÖ Do:**
- **Use shared caches** (default) ‚Äî all instances automatically share caches
- Set resource limits when running 2+ instances simultaneously
- Monitor disk space (shared cache + each build workspace needs 50-120GB)
- Use different `workspace/` directories per instance (already isolated)
- Create shared cache directories once (Step 2 in setup)

### **‚ùå Don't:**
- Run more than 2-3 instances simultaneously (unless you have 32+ GB RAM)
- Share caches between different Yocto versions (e.g., Kirkstone vs. Dunfell) ‚Äî use per-instance caches for different versions
- Share `workspace/` directories (builds must be isolated)
- Delete `.env` file unless you need per-instance caches

---

## **9.4 Example: Two Instances**

**Instance 1** (Project A):
```bash
cd ~/projects/yocto-bbb-project-a
git clone https://github.com/hassansys2/yocto-beaglebone-devcontainer-template.git .
# .env file is included - shared caches configured automatically!
# Open in VSCode ‚Üí Reopen in Container
```

**Instance 2** (Project B):
```bash
cd ~/projects/yocto-bbb-project-b
git clone https://github.com/hassansys2/yocto-beaglebone-devcontainer-template.git .
# .env file is included - shared caches configured automatically!
# Open in VSCode ‚Üí Reopen in Container
```

**No additional configuration needed!** Both instances will:
- ‚úÖ Share sstate cache (faster rebuilds)
- ‚úÖ Share downloads (saves disk/bandwidth)
- ‚úÖ Use separate workspaces (no build conflicts)
- ‚úÖ Work out of the box (`.env` file handles everything)

---

## **9.5 Resource Usage Estimate**

| Scenario | Instances | Total RAM | Total Disk | Build Time |
|----------|-----------|-----------|------------|------------|
| Single instance (shared cache) | 1 | ~12GB | ~80GB | 4 hours |
| Two instances (shared cache - **default**) | 2 | ~24GB | ~100GB | 6-8 hours |
| Two instances (separate cache) | 2 | ~24GB | ~160GB | 8-10 hours |

**Note:** 
- Shared cache (default) saves ~60GB disk space for two instances
- Build times increase due to resource contention when running multiple instances
- First build downloads everything, subsequent builds are much faster due to shared cache

---

# üéâ 10. You're Ready!

You now have a **full Yocto build environment** on macOS using Docker.

---

# üìú License  

This project is licensed under the MIT License ‚Äî free for learning & commercial usage.

See [LICENSE](LICENSE) file for details.

---
